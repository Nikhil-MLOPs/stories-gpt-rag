# ğŸ“š Stories GPT RAG â€“ End-to-End AI Chatbot (Production Ready)

Stories GPT RAG is a **Retrieval-Augmented Generation (RAG)** based AI chatbot that allows users to upload story files (`.txt`, `.pdf`, `.doc`, `.docx`) or paste text, and ask context-based questions. It is built fully from scratch up to cloud deployment and monitoring using:

ğŸ§  FastAPI Â· OpenAI Â· ChromaDB  
ğŸ“Š MLflow Â· DVC  
ğŸ³ Docker Â· GitHub Actions CI/CD  
â˜¸ Kubernetes (k3s) on AWS EC2  
ğŸ“ˆ Prometheus Â· Grafana Monitoring  
ğŸ¨ Pure HTML & CSS (No JavaScript)

---

## ğŸš€ Features

âœ” Upload txt/pdf/doc/docx or paste text  
âœ” Answer context-based questions using RAG pipeline  
âœ” OpenAI Embeddings + ChromaDB for Retrieval  
âœ” Track 5 experiments using MLflow & DVC  
âœ” Dockerized & deployed on AWS EC2  
âœ” Kubernetes (k3s) orchestration  
âœ” GitHub Actions CI/CD automation  
âœ” Monitoring using `/metrics`, Prometheus, Grafana  
âœ” HTML-only UI (No JavaScript)

---

## ğŸ›  Tech Stack

| Layer | Tools |
|-------|-------|
| Backend | FastAPI, Python 3.12.7 |
| LLM/Embedding | OpenAI API |
| Vector Store | ChromaDB |
| Experiment Tracking | MLflow, DVC |
| Deployment | Docker, AWS EC2, k3s |
| CI/CD | GitHub Actions, AWS ECR |
| Monitoring | Prometheus, Grafana |
| UI | HTML & CSS (No JS) |

---

## â–¶ï¸ Setup and Run Locally

### 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
.\.venv\Scripts\activate      # Windows
source .venv/bin/activate     # Linux/Mac

### 2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

### 3ï¸âƒ£ Run FastAPI App

uvicorn app.main:app --reload

### 4ï¸âƒ£ Local Access URLs

Feature	      URL

Upload UI	  http://127.0.0.1:8000/

Chat UI	      http://127.0.0.1:8000/chat-ui

API Docs	  http://127.0.0.1:8000/docs

Metrics	      http://127.0.0.1:8000/metrics

### ğŸ³ Docker Usage

docker build -t stories-gpt-rag:latest .

docker run -p 8000:8000 stories-gpt-rag:latest

Access at â†’ http://localhost:8000

### âš™ï¸ CI/CD (GitHub Actions)

Runs automatically on git push to main:

âœ” Lint (ruff)

âœ” Test (pytest)

âœ” Build Docker image

âœ” Push to AWS ECR

âœ” SSH to EC2 & Restart Kubernetes deployment

File: .github/workflows/ci-cd.yml

### â˜¸ Kubernetes Deployment (k3s on AWS EC2)
cd ~/stories-gpt-rag

git pull origin main

sudo docker build -t stories-gpt-rag:latest .

sudo docker save stories-gpt-rag.tar

sudo k3s ctr images import stories-gpt-rag.tar

cd k8s

sudo kubectl apply -k .

sudo kubectl rollout restart deployment stories-gpt-rag

sudo kubectl get pods

ğŸ”— Application Live URL: http://<EC2_PUBLIC_IP>:30235/

### ğŸ“ˆ Monitoring & Observability
Component	        Access

Metrics Endpoint	/metrics

Grafana Dashboard	http://<EC2_IP>:32000

Grafana Login	    admin / admin123

Prometheus          Scraping	Enabled via ServiceMonitor

Metrics Tracked:

- Request count

- Latency per endpoint

- CPU/Memory usage

- OpenAI API call latency